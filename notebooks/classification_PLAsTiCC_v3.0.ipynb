{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLAsTiCC time series classification\n",
    "This is a ML code that performs classification of the time series from the Photometric LSST Astronomical Time-Series Classification Challenge (PLAsTiCC; https://www.kaggle.com/c/PLAsTiCC-2018). A sample representation of the data is provided after loading the data.\n",
    "\n",
    "Labels are provided for the training set.\n",
    "\n",
    "The steps are the following: <br>\n",
    "1. Import libraries and sample\n",
    "2. Split sub-samples (training, validation, testing)\n",
    "3. Fit with various classifier and check performance\n",
    "4. Compare various classifiers in testing sample\n",
    "\n",
    "The alternative classifiers provide first guesses on the expected accuracy\n",
    "\n",
    "## HISTORY:\n",
    "\n",
    "v3.0:\n",
    "- Using the power spctrum approach in 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, utils, metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_table    = \"/data/deep_learning/database_PLAsTiCC/training_set.csv\"\n",
    "path_to_metadata = \"/data/deep_learning/database_PLAsTiCC/training_set_metadata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "Type: <class 'numpy.ndarray'>\n",
      "Shape: (1421705,)\n",
      "Keys: ('object_id', 'mjd', 'passband', 'flux', 'flux_err', 'detected')\n",
      "\n",
      "Meta data\n",
      "Type: <class 'numpy.ndarray'>\n",
      "Shape: (7848,)\n",
      "Keys: ('object_id', 'ra', 'decl', 'gal_l', 'gal_b', 'ddf', 'hostgal_specz', 'hostgal_photoz', 'hostgal_photoz_err', 'distmod', 'mwebv', 'target')\n"
     ]
    }
   ],
   "source": [
    "table = np.genfromtxt(path_to_table   , delimiter=',', names=True)\n",
    "meta  = np.genfromtxt(path_to_metadata, delimiter=',', names=True)\n",
    "\n",
    "print(\"Data\")\n",
    "print('Type: '  + str(type(table)))\n",
    "print('Shape: ' + str(table.shape))\n",
    "print('Keys: '  + str(table.dtype.names))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Meta data\")\n",
    "print('Type: '  + str(type(meta)))\n",
    "print('Shape: ' + str(meta.shape))\n",
    "print('Keys: '  + str(meta.dtype.names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of targets:               7848\n",
      "Number of labels:                7848\n",
      "\n",
      "Number of possible classes:      14\n",
      "Number of pixels per periodgram: 100\n",
      "Number of bands:                 6\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(meta['target'])\n",
    "classes_str = [\"Class \"+str(int(cl)) for cl in classes]\n",
    "n_classes = len(classes)\n",
    "\n",
    "n_bands  = 6\n",
    "\n",
    "\n",
    "idx_unique = np.where(np.unique(meta['object_id']))[0]\n",
    "# indices of unique target/labels\n",
    "\n",
    "IDs_unique = table['object_id'][idx_unique]\n",
    "# list of target IDs, without repetitions\n",
    "\n",
    "labels_unique = meta['target'][idx_unique]\n",
    "\n",
    "\n",
    "print('Number of targets:               ' + str(len(IDs_unique)))        \n",
    "print('Number of labels:                ' + str(len(labels_unique)))     \n",
    "print()\n",
    "print('Number of possible classes:      ' + str(n_classes))        \n",
    "print('Number of pixels per periodgram: ' + str(n_pixels))\n",
    "print('Number of bands:                 ' + str(n_bands))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting lightcurves into periodgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[=============================                                           ]  41%\r"
     ]
    }
   ],
   "source": [
    "import progressbar\n",
    "\n",
    "from astroML.time_series import lomb_scargle, lomb_scargle_BIC, lomb_scargle_bootstrap\n",
    "\n",
    "periodgrams = np.zeros( (len(IDs_unique) , n_pixels , n_bands ) )\n",
    "# data array < n_targets , image(x,y), bands >\n",
    "# NOTE: The first dimension corresponds to the number of targets <indexed as IDs_unique>\n",
    "\n",
    "n_pixels = 100\n",
    "# numebr of elements in each periodgram\n",
    "\n",
    "\n",
    "bar = progressbar.ProgressBar(maxval=len(IDs_unique), widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "bar.start()\n",
    "# restore:\n",
    "for i in range(len(IDs_unique)):\n",
    "# i = ID index\n",
    "\n",
    "    bar.update(i+1)\n",
    "\n",
    "    table_obj = table[table['object_id'] == IDs_unique[i]]\n",
    "\n",
    "    for b in range(n_bands):\n",
    "\n",
    "        table_obj_b = table_obj[table_obj['passband'] == b]\n",
    "        \n",
    "        # Series for given object and band:\n",
    "        MJD   = table_obj_b['mjd']\n",
    "        F     = table_obj_b['flux']\n",
    "        F_err = table_obj_b['flux_err']\n",
    "        \n",
    "        # log scaling: period = 10 ** np.linspace(-1, 2, n_pixels)\n",
    "        # linear scaling:\n",
    "        period = np.linspace(1, 50, n_pixels)\n",
    "        omega  = 2 * np.pi / period\n",
    "        periodgram_obj_b = lomb_scargle(MJD, F, F_err, omega, generalized=True)\n",
    "\n",
    "        # Normalizing periodgram:\n",
    "        periodgram_obj_b_norm = periodgram_obj_b / np.max(periodgram_obj_b)\n",
    "        \n",
    "        periodgrams[i,:,b] = periodgram_obj_b\n",
    "        \n",
    "bar.finish()        \n",
    "\n",
    "print('Shape of periodgrams: ' + str(periodgrams.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lightcurve(idx_c_i, xrange=None):\n",
    "    \n",
    "    fig, ax = plt.subplots(2, 3, figsize=(12, 6))\n",
    "    \n",
    "    # Iterating over bands:\n",
    "    for i in range(3):\n",
    "        for j in range(2):\n",
    "            b = 2*i+j\n",
    "            \n",
    "            table_obj = table[table['object_id'] == IDs_unique[idx_c_i]]\n",
    "\n",
    "            table_obj_b = table_obj[table_obj['passband'] == b]\n",
    "        \n",
    "            # Series for given object and band:\n",
    "            MJD   = table_obj_b['mjd']\n",
    "            F     = table_obj_b['flux']\n",
    "            F_err = table_obj_b['flux_err']\n",
    "                            \n",
    "            ax[j,i].errorbar(MJD,\n",
    "                             F,\n",
    "                             yerr=F_err,\n",
    "                             fmt='o', marker='.', ecolor='k', markersize=10)\n",
    "            \n",
    "            ax[j,i].set_title('Filter '+str(int(b)))\n",
    "            \n",
    "            if not xrange is None:\n",
    "                ax[j,i].set_xlim(xrange[0], xrange[1])\n",
    "                    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_periodogram(idx_c_i):\n",
    "    \n",
    "    fig, ax = plt.subplots(2, 3, figsize=(12, 6))\n",
    "    \n",
    "    # Iterating over bands:\n",
    "    for i in range(3):\n",
    "        for j in range(2):\n",
    "            b = 2*i+j\n",
    "\n",
    "            periodgram_obj_b = periodgrams[idx_c_i,:,b]\n",
    "            \n",
    "            period = np.arange(len(periodgram_obj_b))\n",
    "\n",
    "            ax[j,i].plot(period, periodgram_obj_b, '-', c='black', lw=1, zorder=1)\n",
    "                        \n",
    "            ax[j,i].set_title('Filter '+str(int(b)))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "            \n",
    "for c in range(n_classes):\n",
    "# c = class index\n",
    "    \n",
    "    print(\"Class \" + str(int(classes[c])))\n",
    "\n",
    "    \n",
    "    #idx = np.where(table_obj['passband'] == i_filter)[0]\n",
    "\n",
    "    idx_c = np.where(meta['target'] == classes[c])[0]\n",
    "    # indexes of targets of class = c\n",
    "    \n",
    "    idx_c_0 = idx_c[0]\n",
    "    # index of first target of class = c\n",
    " \n",
    "    plot_lightcurve(idx_c_0)\n",
    "    plot_periodogram(idx_c_0)\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the samples\n",
    "shuffled_indexes = np.arange(len(IDs_unique))\n",
    "np.random.shuffle(shuffled_indexes)\n",
    "\n",
    "# To reduce the sample size (for testing purposes):\n",
    "# remove: shuffled_indexes = shuffled_indexes[0:1000]\n",
    "# remove: n_samples = len(shuffled_indexes)\n",
    "\n",
    "periodgrams_shuffled = periodgrams[shuffled_indexes]\n",
    "labels_shuffled = list(np.array(labels_unique)[shuffled_indexes])\n",
    "\n",
    "n_samples = len(IDs_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data   = periodgrams_shuffled\n",
    "labels = labels_shuffled\n",
    "\n",
    "# Splitting in training, validation, and test samples:\n",
    "data_train = data[:8 * n_samples // 10] # i.e. 80% training\n",
    "labels_train = labels[:8 * n_samples // 10]\n",
    "\n",
    "data_valid = data[8 * n_samples // 10:9 * n_samples // 10] # i.e. 10% validation (80->90%)\n",
    "labels_valid = labels[8 * n_samples // 10:9 * n_samples // 10]\n",
    "\n",
    "data_test = data[9 * n_samples // 10:] # i.e. 10% testing (90->100%)\n",
    "labels_test = labels[9 * n_samples // 10:]\n",
    "\n",
    "#n_train_spiral = len([x for x in labels_train if x == 'spiral'])\n",
    "#n_train_ellipt = len([x for x in labels_train if x == 'ellipt'])\n",
    "\n",
    "#n_valid_spiral = len([x for x in labels_valid if x == 'spiral'])\n",
    "#n_valid_ellipt = len([x for x in labels_valid if x == 'ellipt'])\n",
    "\n",
    "#n_test_spiral = len([x for x in labels_test if x == 'spiral'])\n",
    "#n_test_ellipt = len([x for x in labels_test if x == 'ellipt'])\n",
    "\n",
    "print(\"Sample Summary\")\n",
    "print(\"________________________\")\n",
    "print(\"Total images     | %5s\" % len(data))\n",
    "print(\"-----------------|------\")\n",
    "print(\" '-> Training    | %5s\" % len(data_train))\n",
    "#print(\"      '-> spiral | %5s (%.1f%%)\" % (n_train_spiral , (n_train_spiral/len(data_train)*100.)))\n",
    "#print(\"      '-> ellipt | %5s (%.1f%%)\" % (n_train_ellipt , (n_train_ellipt/len(data_train)*100.)))\n",
    "print(\"-----------------|------\")\n",
    "print(\" '-> Validation  | %5s\" % len(data_valid))\n",
    "#print(\"      '-> spiral | %5s (%.1f%%)\" % (n_valid_spiral , (n_valid_spiral/len(data_valid)*100.)))\n",
    "#print(\"      '-> ellipt | %5s (%.1f%%)\" % (n_valid_ellipt , (n_valid_ellipt/len(data_valid)*100.)))\n",
    "print(\"-----------------|------\")\n",
    "print(\" '-> Test        | %5s\" % len(data_test))\n",
    "#print(\"      '-> spiral | %5s (%.1f%%)\" % (n_test_spiral , (n_test_spiral/len(data_test)*100.)))\n",
    "#print(\"      '-> ellipt | %5s (%.1f%%)\" % (n_test_ellipt , (n_test_ellipt/len(data_test)*100.)))\n",
    "\n",
    "print('')\n",
    "print('Compare these values with the accuracy of each classifier')\n",
    "print('If accuracies are similar to the demographics, the classifier is only mirroring the data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to 1 band for other classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to 1 band:\n",
    "\n",
    "# Using only 1 band, and compressing dimensions into 1:\n",
    "data_train_1D = data_train[:,:,5].reshape(len(data_train),n_pixels)\n",
    "data_valid_1D = data_valid[:,:,5].reshape(len(data_valid),n_pixels)\n",
    "\n",
    "labels_train_1D = labels_train\n",
    "labels_valid_1D = labels_valid \n",
    "\n",
    "print('Shape of train 1D       | ' + str(data_train_1D.shape))\n",
    "print('Shape of valid 1D       | ' + str(data_valid_1D.shape))\n",
    "\n",
    "print('Shape of label train 1D | ' + str(len(labels_train_1D)))\n",
    "print('Shape of label valid 1D | ' + str(len(labels_valid_1D)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## > Using scikit-learn / SVM classifier\n",
    "Classical Support Vector Machines classifier\n",
    "NOTE: The non-linear kernel is extremely slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier\n",
    "model_svc = LinearSVC()\n",
    "#model_svc = NuSVC()\n",
    "model_svc.fit(data_train_1D, labels_train_1D)\n",
    "\n",
    "# Comparisong with prediction\n",
    "predicted = model_svc.predict(data_valid_1D)\n",
    "\n",
    "print(\"Classification report for %s:\\n%s\\n\"\n",
    "      % (model_svc, metrics.classification_report(labels_valid_1D, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(labels_valid_1D, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## > Using scikit learn / Random Forests\n",
    "A scikit-learn bagging classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier\n",
    "model_RF = RandomForestClassifier()\n",
    "model_RF.fit(data_train_1D, labels_train_1D)\n",
    "\n",
    "# Comparisong with prediction\n",
    "predicted = model_RF.predict(data_valid_1D)\n",
    "\n",
    "print(\"Classification report for %s:\\n%s\\n\"\n",
    "      % (model_RF, metrics.classification_report(labels_valid_1D, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(labels_valid_1D, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## > Building a Keras Neural Network classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Convolution1D\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import MaxPool1D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from tensorflow.contrib.layers import maxout\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bands = data_train.shape[-1]\n",
    "\n",
    "# >> One hot encoding the class values to tranform the vector of class integers into a binary matrix:\n",
    "int_enc = LabelEncoder()\n",
    "labels_train_int = int_enc.fit_transform(labels_train)\n",
    "labels_valid_int = int_enc.fit_transform(labels_valid)\n",
    "labels_test_int  = int_enc.fit_transform(labels_test)\n",
    "\n",
    "labels_train_int = np.expand_dims(labels_train_int, axis=1)\n",
    "labels_valid_int = np.expand_dims(labels_valid_int, axis=1)\n",
    "labels_test_int  = np.expand_dims(labels_test_int, axis=1)\n",
    "\n",
    "# Replicating the classification for all the bands:\n",
    "#labels_train_int = np.repeat(labels_train_int[:,np.newaxis], n_bands, 1)\n",
    "#labels_valid_int = np.repeat(labels_valid_int[:,np.newaxis], n_bands, 1)\n",
    "#labels_test_int  = np.repeat(labels_test_int[:,np.newaxis], n_bands, 1)\n",
    "\n",
    "oh_enc = OneHotEncoder(sparse=False)\n",
    "labels_train_ohe = oh_enc.fit_transform(labels_train_int)\n",
    "labels_valid_ohe = oh_enc.fit_transform(labels_valid_int)\n",
    "labels_train_ohe = oh_enc.fit_transform(labels_test_int)\n",
    "\n",
    "# uniques, labels_valid = np.unique(labels_valid, return_inverse=True)\n",
    "labels_train_cat = np_utils.to_categorical(labels_train_int)\n",
    "labels_valid_cat = np_utils.to_categorical(labels_valid_int)\n",
    "labels_test_cat  = np_utils.to_categorical(labels_test_int)\n",
    "\n",
    "#n_classes = labels_valid_ohe.shape[1]\n",
    "#n_classes = 15\n",
    "# must hard-code this one\n",
    "\n",
    "print(\"Labels formats for convolutional layers:\")\n",
    "\n",
    "print(\"Train      int label format (?, ?, n_samples, n_channels)         | \", labels_train_int.shape)\n",
    "print(\"Validation int label format (?, ?, n_samples, n_channels)         | \", labels_valid_int.shape)\n",
    "print(\"Test       int label format (?, ?, n_samples, n_channels)         | \", labels_test_int.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pixels = data_train.shape[1] \n",
    "\n",
    "# Formatting data for convolutional layer:\n",
    "n_train_targets = data_train.shape[0]\n",
    "n_valid_targets = data_valid.shape[0]\n",
    "n_test_targets  = data_test.shape[0]\n",
    "\n",
    "#labels_train_int_4D = np.expand_dims(labels_train_int   , axis=0)\n",
    "#labels_train_int_4D = np.expand_dims(labels_train_int_4D, axis=0)\n",
    "#labels_valid_int_4D = np.expand_dims(labels_valid_int   , axis=0)\n",
    "#labels_valid_int_4D = np.expand_dims(labels_valid_int_4D, axis=0)\n",
    "#labels_test_int_4D  = np.expand_dims(labels_test_int    , axis=0)\n",
    "#labels_test_int_4D  = np.expand_dims(labels_test_int_4D , axis=0)\n",
    "\n",
    "\n",
    "print(\"Data formats for convolutional layers:\")\n",
    "\n",
    "print(\"Train      4D data format (n_samples,size_x, size_y, n_channels) | \", data_train.shape)\n",
    "print(\"Validation 4D data format (n_samples,size_x, size_y, n_channels) | \", data_valid.shape)\n",
    "print(\"Test       4D data format (n_samples,size_x, size_y, n_channels) | \", data_test.shape)\n",
    "\n",
    "#print(\"Train      4D label format (?, ?, n_samples, n_channels)         | \", labels_train_int_4D.shape)\n",
    "#print(\"Validation 4D label format (?, ?, n_samples, n_channels)         | \", labels_valid_int_4D.shape)\n",
    "#print(\"Test       4D label format (?, ?, n_samples, n_channels)         | \", labels_test_int_4D.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying with less bands:\n",
    "\n",
    "n_bands = 6\n",
    "\n",
    "data_train = data_train[:,:,:n_bands]\n",
    "data_valid = data_valid[:,:,:n_bands]\n",
    "data_test  = data_test [:,:,:n_bands]\n",
    "\n",
    "if (n_bands == 1):\n",
    "    data_train = np.expand_dims(data_train, axis=3)\n",
    "    data_valid = np.expand_dims(data_valid, axis=3)\n",
    "    data_test  = np.expand_dims(data_test, axis=3)\n",
    "\n",
    "\n",
    "print(\"Data formats for convolutional layers - 1 band:\")\n",
    "\n",
    "print(\"Train      4D data format (n_samples,size_x, size_y, n_channels) | \", data_train.shape)\n",
    "print(\"Validation 4D data format (n_samples,size_x, size_y, n_channels) | \", data_valid.shape)\n",
    "print(\"Test       4D data format (n_samples,size_x, size_y, n_channels) | \", data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional 1D layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Conv = keras.Sequential([\n",
    "                          #keras.layers.Conv1D(8, kernel_size=4, strides=2, padding='same',\n",
    "                          #                    activation=tf.nn.relu, input_shape=(n_pixels,n_bands)),\n",
    "                          #keras.layers.MaxPool1D(pool_size=2, strides=2),\n",
    "                          #keras.layers.Conv1D(8, kernel_size=4, strides=2, padding='same',\n",
    "                          #                    activation=tf.nn.relu, input_shape=(n_pixels,n_bands)),\n",
    "                          #keras.layers.MaxPool1D(pool_size=2, strides=2),\n",
    "                          #keras.layers.Dropout(0.3),\n",
    "                          keras.layers.Dense(20, activation=tf.nn.sigmoid, input_shape=(n_pixels,n_bands)),\n",
    "                          keras.layers.Dense(50, activation=tf.nn.sigmoid),\n",
    "                          keras.layers.Dense(20, activation=tf.nn.sigmoid),\n",
    "                          keras.layers.Dropout(0.3),\n",
    "                          keras.layers.Flatten(),\n",
    "                          keras.layers.Dense(n_classes, activation=tf.nn.sigmoid)\n",
    "                          ])\n",
    "\n",
    "model_Conv.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.001), \n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model_Conv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_Conv = model_Conv.fit(data_train, labels_train_int, validation_data=(data_valid, labels_valid_int),\n",
    "                    epochs=10, batch_size=250, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# > Model evolution:\n",
    "plt.plot(history_Conv.history['loss'])\n",
    "plt.plot(history_Conv.history['val_loss'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('Number of different classes in train sample: ' + str(len(np.unique(labels_train))))\n",
    "print('Number of different classes in valid sample: ' + str(len(np.unique(labels_valid))))\n",
    "print('Number of different classes in test  sample: ' + str(len(np.unique(labels_test))))\n",
    "print('')\n",
    "print('WARNING: A disply error can occur if the represented classes differ in the train/valid/test sets.')\n",
    "print('         If so, increase the samples.')\n",
    "\n",
    "\n",
    "# > Comparison with predictions:\n",
    "labels_pred_float_Conv = model_Conv.predict(data_test)\n",
    "\n",
    "labels_pred_Conv = int_enc.inverse_transform(labels_pred_float_Conv.argmax(1))\n",
    "# reversing one hot encoding\n",
    "\n",
    "\n",
    "print(\"Classification report for %s:\\n%s\\n\"\n",
    "      % (model_Conv, metrics.classification_report(labels_test, labels_pred_Conv)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(labels_test, labels_pred_Conv))\n",
    "\n",
    "\n",
    "# > Plotting a few misclassified images (wrong labels):\n",
    "\n",
    "print('Examples of misclassified images:')\n",
    "\n",
    "# equivalent(?): indexes_wrong = np.nonzero(labels_pred_Conv != labels_test_int)\n",
    "indexes_wrong = [labels_pred_Conv != labels_test]\n",
    "# indexes of misclassified objects <indexed within range(data_test) or range(labels_pred_*)>\n",
    "\n",
    "labels_pred_int_Conv = labels_pred_float_Conv.argmax(1)\n",
    "\n",
    "data_test_wrong            = data_test[indexes_wrong]\n",
    "labels_test_int_wrong      = labels_test_int[indexes_wrong]\n",
    "labels_pred_int_Conv_wrong = labels_pred_int_Conv[indexes_wrong]\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "plt.subplots_adjust(hspace=0.25,wspace=0.05)\n",
    "\n",
    "       \n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.axis('off')\n",
    "    peridogram_wrong = (data_test_wrong[i,:,0].reshape(n_pixels))\n",
    "    # using only first band in array\n",
    "    \n",
    "    vmin,vmax = np.percentile(image_wrong.flatten(),[1,99])\n",
    "    # min and max are set as percentiles for each image\n",
    "\n",
    "    plt.plot(peridogram_wrong)\n",
    "    plt.title('label = %s | pred = %s' % (labels_shuffled[labels_test_int_wrong[i][0]] , labels_shuffled[labels_pred_int_Conv_wrong[i]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/\n",
    "\n",
    "## Using Tensorflow\n",
    "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/neural_network_raw.ipynb\n",
    "\n",
    "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/neural_network.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final check \n",
    "Using the test sample for a final check of the various algorithms used above. \n",
    "\n",
    "Checking if their performance is as good as it is reported in the validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"====================================================================================\")\n",
    "# Predictions for SVM:\n",
    "predicted = model_svc.predict(data_test_1D)\n",
    "print(\"Classification report for %s:\\n%s\\n\"\n",
    "      % (model_svc, metrics.classification_report(labels_test, predicted)))\n",
    "print(\"Confusion matrix:\\n%s \\n\" % metrics.confusion_matrix(labels_test, predicted))\n",
    "print(\"====================================================================================\")\n",
    "\n",
    "# Predictions for LogisticRegression:\n",
    "#predicted = model_svc.predict(data_test)\n",
    "#print(\"Classification report for %s:\\n%s\\n\"\n",
    "#      % (model_lrc, metrics.classification_report(labels_test, predicted)))\n",
    "#print(\"Confusion matrix:\\n%s \\n\" % metrics.confusion_matrix(labels_test, predicted))\n",
    "#print(\"====================================================================================\")\n",
    "\n",
    "# Predictions for MLP:\n",
    "#predicted = model_MLP.predict(data_test)\n",
    "#print(\"Classification report for %s:\\n%s\\n\"\n",
    "#      % (model_MLP, metrics.classification_report(labels_test, predicted)))\n",
    "#print(\"Confusion matrix:\\n%s \\n\" % metrics.confusion_matrix(labels_test, predicted))\n",
    "#print(\"====================================================================================\")\n",
    "\n",
    "# Predictions for RandomForests:\n",
    "predicted = model_RF.predict(data_test_1D)\n",
    "print(\"Classification report for %s:\\n%s\\n\"\n",
    "      % (model_RF, metrics.classification_report(labels_test, predicted)))\n",
    "print(\"Confusion matrix:\\n%s \\n\" % metrics.confusion_matrix(labels_test, predicted))\n",
    "print(\"====================================================================================\")\n",
    "\n",
    "# Predictions for Keras 1D Neural Network:\n",
    "#labels_pred_float_1D = model_1D.predict(data_test)\n",
    "#labels_pred_1D = int_enc.inverse_transform(labels_pred_float_1D.argmax(1))\n",
    "#print(\"Classification report for %s:\\n%s\\n\"\n",
    "#      % (model_1D, metrics.classification_report(labels_test, labels_pred_1D)))\n",
    "#print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(labels_test, labels_pred_1D))\n",
    "#print(\"====================================================================================\")\n",
    "\n",
    "# Predictions for Keras Convolutional Neural Network:\n",
    "labels_pred_float_Conv = model_Conv.predict(data_test)\n",
    "labels_pred_Conv = int_enc.inverse_transform(labels_pred_float_Conv.argmax(1))\n",
    "print(\"Classification report for %s:\\n%s\\n\"\n",
    "      % (model_Conv, metrics.classification_report(labels_test, labels_pred_Conv)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(labels_test, labels_pred_Conv))\n",
    "print(\"====================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the trained models\n",
    "Using pickle we can save the trained models to use them in a later time w/o the need to re-train them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#import os\n",
    "\n",
    "## saving the models to disk\n",
    "#folder_saved = \"saved_models/v3.0\"\n",
    "\n",
    "#model_labels = ['SVC','LogReg','MLP','RandFor','Ker_1D','Ker_Conv']\n",
    "#models = ['model_SVC','model_LR','model_MLP','model_RF','model_1D','model_Conv']\n",
    "\n",
    "#if not os.path.exists(folder_saved):\n",
    "#    os.makedirs(folder_saved)\n",
    "    \n",
    "#for modstr,model in zip(model_labels,models):\n",
    "#    pickle.dump(model, open(folder_saved+'/galaxy_classification_SDSS_v1_'+modstr+'.sav', 'wb'))\n",
    "\n",
    "\n",
    "# To load the models at a later time:\n",
    "# loaded_model = pickle.load(open(filename, 'rb'))\n",
    "# result = loaded_model.score(X_test, Y_test)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
