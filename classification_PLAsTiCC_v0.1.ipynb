{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLAsTiCC time series classification\n",
    "This is a ML code that performs classification of the time series from the Photometric LSST Astronomical Time-Series Classification Challenge (PLAsTiCC; https://www.kaggle.com/c/PLAsTiCC-2018). A sample representation of the data is provided after loading the data.\n",
    "\n",
    "Labels are provided for the training set.\n",
    "\n",
    "The steps are the following: <br>\n",
    "1. Import libraries and sample\n",
    "2. Split sub-samples (training, validation, testing)\n",
    "3. Fit with various classifier and check performance\n",
    "4. Compare various classifiers in testing sample\n",
    "\n",
    "The alternative classifiers provide first guesses on the expected accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, utils, metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"/data/deep_learning/database_PLAsTiCC/training_data_v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = sorted(glob.glob(path_to_data + \"/*.npy\"))\n",
    "\n",
    "\n",
    "# > Loading series / labels:\n",
    "# <indexed as ID array>\n",
    "\n",
    "#restore:\n",
    "n_files = len(files)\n",
    "#debug: n_files = 1000\n",
    "\n",
    "IDs   = []\n",
    "# ID number (without \"id-\")\n",
    "bands = []\n",
    "# band name (without \"band-\") [0 .. 5]\n",
    "\n",
    "series = []\n",
    "labels = []\n",
    "\n",
    "data_uneven = []\n",
    "# list of data <image(x,y), band>\n",
    "# NOTE: the images in data can have different sizes\n",
    "#       Filled by one file per entry, independently from ID\n",
    "\n",
    "Duration_MJDs = []\n",
    "# array of duration of light curves\n",
    "\n",
    "for i in range(n_files):\n",
    "\n",
    "    tmp = np.load(files[i])\n",
    "\n",
    "    IDs.append(files[i].split(\"/\")[-1].split(\"_\")[0].strip(\"id-\"))\n",
    "    bands.append(int(files[i].split(\"/\")[-1].split(\"_\")[1].strip(\"band-\")))\n",
    "\n",
    "    band = int(bands[-1])\n",
    "    # current band index [0 .. 6]\n",
    "    \n",
    "    series.append(tmp)\n",
    "    labels.append(files[i].split(\"/\")[-1].split(\"-\")[-1].strip(\".npy\"))\n",
    "\n",
    "    \n",
    "    # > Loading single data files:\n",
    "    #\n",
    "    # Data format:\n",
    "    #\n",
    "    # Modified Juilian Date, Flux, err_Flux, Detected (flag)\n",
    "\n",
    "    MJD = series[-1][:,0]\n",
    "    # Modified Julian Date\n",
    "    \n",
    "    F = series[-1][:,1]\n",
    "    # flux\n",
    "    \n",
    "    F_err = series[-1][:,2]\n",
    "    # flux error\n",
    "    \n",
    "    \n",
    "    Duration_MJDs.append(MJD[-1] - MJD[0])\n",
    "\n",
    "    # > Converting data into a matrix of flux \"ratios\"\n",
    "    #\n",
    "    #   [1             , (F_1 / F_2)  , ..       , ..       , (F_1 / F_N)  ]\n",
    "    #   [(F_2 / F_1)   , 1            , ..       , ..       , (F_2 / F_N)  ]\n",
    "    #   [..            , ..           , 1        , ..       , ..           ]\n",
    "    #   [(F_N-1 / F_1) , ..           , ..       , 1        , (F_N-1 / F_N)]\n",
    "    #   [(F_N / F_1)   , ..           , ..       , ..       , 1            ]\n",
    "\n",
    "    \n",
    "    data_i = np.zeros((len(MJD),len(MJD),6))\n",
    "    # flux ratio matrix for source i\n",
    "    # NOTE: organizing data in a 2D matrix with shape [ len(array) ,  len(array) , channels(= n_bands) ]\n",
    "\n",
    "    \n",
    "    for j in range(len(MJD)):\n",
    "    # j = row index\n",
    "      \n",
    "        data_i[j,:,band] = ( F[j] / F )\n",
    "\n",
    "              \n",
    "    # > Mirroring matrix across the diagonal:\n",
    "    data_i_band = data_i[:,:,band]\n",
    "    indexes_lower = np.tril_indices(data_i_band.shape[0], -1)\n",
    "    # indexes of lower part of matrix\n",
    "    data_i_band[indexes_lower] = data_i_band.T[indexes_lower]\n",
    "    # mirroring\n",
    "    \n",
    "    data_i[:,:,band] = data_i_band\n",
    "\n",
    "    data_uneven.append((data_i[:,:,band] , band))\n",
    "\n",
    "\n",
    "data_uneven = np.array(data_uneven)\n",
    "    \n",
    "Duration_MJDs = np.array(Duration_MJDs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some example images of galaxies\n",
    "images_bands_and_labels = list(zip(data_uneven, labels))\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "for index in range(10):\n",
    "    \n",
    "    image = images_bands_and_labels[index][0][0]\n",
    "    label = images_bands_and_labels[index][1]\n",
    "    \n",
    "    plt.subplot(2, 5, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, vmin=0.5*np.min(image), vmax=0.8*np.max(image), interpolation='nearest')\n",
    "    plt.title('class | ' + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "\n",
    "'''\n",
    "Interpolation Example.\n",
    "\n",
    "mymin,mymax = 0,3\n",
    "X = np.linspace(mymin,mymax,4)\n",
    "Y = np.linspace(mymin,mymax,4)\n",
    "\n",
    "x,y = np.meshgrid(X,Y)\n",
    "\n",
    "test = np.array([[ 1.2514318 ,  1.25145821,  1.25148472,  1.25151133],\n",
    "       [ 1.25087456,  1.25090105,  1.25092764,  1.25095435],\n",
    "       [ 1.25031581,  1.25034238,  1.25036907,  1.25039586],\n",
    "       [ 1.24975557,  1.24978222,  1.24980898,  1.24983587]])\n",
    "\n",
    "f = interpolate.interp2d(x,y,test,kind='cubic')\n",
    "\n",
    "# use linspace so your new range also goes from 0 to 3, with 8 intervals\n",
    "Xnew = np.linspace(mymin,mymax,8)\n",
    "Ynew = np.linspace(mymin,mymax,8)\n",
    "\n",
    "test8x8 = f(Xnew,Ynew)\n",
    "'''  \n",
    "\n",
    "\n",
    "# >> Resampling images over common canvases\n",
    "\n",
    "Duration_MAX = np.max(Duration_MJDs)\n",
    "# longest duration of time series [MDJ]\n",
    "\n",
    "deltaT_per_pixel = Duration_MAX / 256.\n",
    "# time interval within a pixel of the new, resampled images [MJD/pixel]\n",
    "# NOTE: this is defined with respect to the largest duration\n",
    "# NOTE: final images will have 256 x 256 pixels\n",
    "\n",
    "\n",
    "\n",
    "# > Resampling images to contain the same delta_t per pixel\n",
    "\n",
    "data_resamp = []\n",
    "# creating \"canvas\" array to be filled by loaded data (leaving 0s to pad missing data) <image(x,y), band>\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "for i,(image,label) in enumerate(data_uneven):\n",
    "\n",
    "    pixels_image = int(np.ceil( Duration_MJDs[i] / deltaT_per_pixel ))\n",
    "    # number of pixels that the current image will cover in the resampled image\n",
    "\n",
    "    X = np.linspace(0,image.shape[0]-1,image.shape[0])\n",
    "    Y = np.linspace(0,image.shape[1]-1,image.shape[1])\n",
    "\n",
    "    x,y = np.meshgrid(X,Y)\n",
    "\n",
    "\n",
    "    f = interpolate.interp2d(x,y,image,kind='linear')\n",
    "    \n",
    "    # use linspace so new range also goes from 0 to image.shape[*]-1, with pixels_image intervals\n",
    "    X_resamp = np.linspace(0,image.shape[0]-1,pixels_image)\n",
    "    Y_resamp = np.linspace(0,image.shape[1]-1,pixels_image)\n",
    "\n",
    "    image_resamp = f(X_resamp,Y_resamp)\n",
    "\n",
    "    data_resamp.append((image_resamp,band))\n",
    "\n",
    "    # Plotting a few resampled images:\n",
    "    if (i < 10):\n",
    "                       \n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image_resamp, cmap=plt.cm.gray_r, vmin=0.5*np.min(image_resamp), vmax=0.8*np.max(image_resamp), interpolation='nearest')\n",
    "        plt.title(str(image.shape) + \" -> \" + str(image_resamp.shape))\n",
    "\n",
    "data_resamp = np.array(data_resamp)\n",
    "\n",
    "    \n",
    "# > Padding data to match maximum number of points among all data series, and normalizing\n",
    "\n",
    "data_rebin = np.zeros( (len(data_resamp) , 256 , 256 ) )\n",
    "# creating \"canvas\" array to be filled by loaded data (leaving 0s to pad missing data) <n_images , image(x,y)>\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "for i,(image_resamp,band) in enumerate(data_resamp):\n",
    "        \n",
    "    image_resamp_norm = (image_resamp + abs(np.min(image_resamp))) / np.max(image_resamp + abs(np.min(image_resamp)))\n",
    "    \n",
    "    data_rebin[i,0:image_resamp_norm.shape[0],0:image_resamp_norm.shape[1]] = image_resamp_norm\n",
    "    \n",
    "    image_rebin = data_rebin[i,:,:]\n",
    "\n",
    "    \n",
    "    # Plotting a few rebinned images:\n",
    "    \n",
    "    if (i < 10):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image_rebin, cmap=plt.cm.gray_r, vmin=-0.05, vmax=1.2, interpolation='nearest')\n",
    "        plt.title(str(image_resamp_norm.shape) + \" -> (256,256)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >> Organizing images by channels (i.e. band) corresponding to the same object ID\n",
    "\n",
    "IDs_unique = np.unique(IDs)\n",
    "# list of target IDs, without repetitions\n",
    "\n",
    "labels_unique = ['' for x in range(len(IDs_unique))]\n",
    "# list of target labels, without repetitions <indexed as IDs_unique>\n",
    "\n",
    "print('Number of individual targets: ' + str(len(IDs_unique)))\n",
    "\n",
    "\n",
    "data = np.zeros( (len(IDs_unique) , 256 , 256 , 6 ) )\n",
    "# data array < n_targets , image(x,y), bands >\n",
    "# NOTE: The first dimension corresponds to the number of targets <indexed as IDs_unique>\n",
    "\n",
    "for i, image_rebin in enumerate(data_rebin):\n",
    "# i = image index\n",
    "\n",
    "    (t,) = np.where(IDs_unique == IDs[i])\n",
    "    t = t[0]\n",
    "    # t = index of target in array of unique IDs\n",
    "    # NOTE: manipulation is because np.where returns a 1-value array instead of a variable\n",
    "\n",
    "    labels_unique[t] = labels[i]\n",
    "    # NOTE: This is over-written 6 times (once per band)\n",
    "    #       But it doesn't matter since all bands share the same classification \n",
    "\n",
    "    band = int(bands[i])\n",
    "    # NOTE: \"bands\" is indexed as \"IDs\"\n",
    "\n",
    "    data[t,:,:,band] = image_rebin\n",
    "    # loading image in corresponding unique (object ID, band) slot\n",
    "\n",
    "print('Shape of data: ' + str(data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display examples of data series of same class over different bands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,25))\n",
    "\n",
    "for t,data_t in (enumerate(data)):\n",
    "# t = target index\n",
    "    \n",
    "    for band in range(data_t.shape[2]):\n",
    "    # iterating over the bands for target t\n",
    "    # (they could in principle be different for different targets)\n",
    "\n",
    "        band = int(band)\n",
    "        \n",
    "        image_t_b = data[t,:,:,band]\n",
    "        # image corresponding to a given target t and band\n",
    "\n",
    "        # Plotting:\n",
    "\n",
    "        p = t*6 + band\n",
    "        # p = plot counter\n",
    "        \n",
    "        plt.subplot(10, 6, p + 1)\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.imshow(image_t_b, cmap=plt.cm.gray_r, vmin=-0.05, vmax=1.2, interpolation='nearest')\n",
    "        plt.title(str(IDs_unique[t]) + ' | b ' + str(band) + ' | cl ' + labels_unique[t])\n",
    "        \n",
    "    if p == 17: break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the samples\n",
    "shuffled_indexes = np.arange(len(IDs_unique))\n",
    "np.random.shuffle(shuffled_indexes)\n",
    "\n",
    "# To reduce the sample size (for testing purposes):\n",
    "# remove: shuffled_indexes = shuffled_indexes[0:1000]\n",
    "# remove: n_samples = len(shuffled_indexes)\n",
    "\n",
    "data = data[shuffled_indexes]\n",
    "labels_unique = list(np.array(labels_unique)[shuffled_indexes])\n",
    "\n",
    "n_samples = len(IDs_unique)\n",
    "\n",
    "# purging memory from variabls not used anymore\n",
    "del series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting in training, validation, and test samples:\n",
    "data_train = data[:8 * n_samples // 10] # i.e. 80% training\n",
    "labels_train = labels_unique[:8 * n_samples // 10]\n",
    "\n",
    "data_valid = data[8 * n_samples // 10:9 * n_samples // 10] # i.e. 10% validation (80->90%)\n",
    "labels_valid = labels_unique[8 * n_samples // 10:9 * n_samples // 10]\n",
    "\n",
    "data_test = data[9 * n_samples // 10:] # i.e. 10% testing (90->100%)\n",
    "labels_test = labels_unique[9 * n_samples // 10:]\n",
    "\n",
    "#n_train_spiral = len([x for x in labels_train if x == 'spiral'])\n",
    "#n_train_ellipt = len([x for x in labels_train if x == 'ellipt'])\n",
    "\n",
    "#n_valid_spiral = len([x for x in labels_valid if x == 'spiral'])\n",
    "#n_valid_ellipt = len([x for x in labels_valid if x == 'ellipt'])\n",
    "\n",
    "#n_test_spiral = len([x for x in labels_test if x == 'spiral'])\n",
    "#n_test_ellipt = len([x for x in labels_test if x == 'ellipt'])\n",
    "\n",
    "print(\"Sample Summary\")\n",
    "print(\"________________________\")\n",
    "print(\"Total images     | %5s\" % len(data))\n",
    "print(\"-----------------|------\")\n",
    "print(\" '-> Training    | %5s\" % len(data_train))\n",
    "#print(\"      '-> spiral | %5s (%.1f%%)\" % (n_train_spiral , (n_train_spiral/len(data_train)*100.)))\n",
    "#print(\"      '-> ellipt | %5s (%.1f%%)\" % (n_train_ellipt , (n_train_ellipt/len(data_train)*100.)))\n",
    "print(\"-----------------|------\")\n",
    "print(\" '-> Validation  | %5s\" % len(data_valid))\n",
    "#print(\"      '-> spiral | %5s (%.1f%%)\" % (n_valid_spiral , (n_valid_spiral/len(data_valid)*100.)))\n",
    "#print(\"      '-> ellipt | %5s (%.1f%%)\" % (n_valid_ellipt , (n_valid_ellipt/len(data_valid)*100.)))\n",
    "print(\"-----------------|------\")\n",
    "print(\" '-> Test        | %5s\" % len(data_test))\n",
    "#print(\"      '-> spiral | %5s (%.1f%%)\" % (n_test_spiral , (n_test_spiral/len(data_test)*100.)))\n",
    "#print(\"      '-> ellipt | %5s (%.1f%%)\" % (n_test_ellipt , (n_test_ellipt/len(data_test)*100.)))\n",
    "\n",
    "print('')\n",
    "print('Compare these values with the accuracy of each classifier')\n",
    "print('If accuracies are similar to the demographics, the classifier is only mirroring the data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Building a Keras Neural Network classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Convolution1D\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from tensorflow.contrib.layers import maxout\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bands = data_train.shape[-1]\n",
    "\n",
    "# >> One hot encoding the class values to tranform the vector of class integers into a binary matrix:\n",
    "int_enc = LabelEncoder()\n",
    "labels_train_int = int_enc.fit_transform(labels_train)\n",
    "labels_valid_int = int_enc.fit_transform(labels_valid)\n",
    "labels_test_int  = int_enc.fit_transform(labels_test)\n",
    "\n",
    "labels_train_int = np.expand_dims(labels_train_int, axis=1)\n",
    "labels_valid_int = np.expand_dims(labels_valid_int, axis=1)\n",
    "labels_test_int  = np.expand_dims(labels_test_int, axis=1)\n",
    "\n",
    "# Replicating the classification for all the bands:\n",
    "#labels_train_int = np.repeat(labels_train_int[:,np.newaxis], n_bands, 1)\n",
    "#labels_valid_int = np.repeat(labels_valid_int[:,np.newaxis], n_bands, 1)\n",
    "#labels_test_int  = np.repeat(labels_test_int[:,np.newaxis], n_bands, 1)\n",
    "\n",
    "oh_enc = OneHotEncoder(sparse=False)\n",
    "labels_train_ohe = oh_enc.fit_transform(labels_train_int)\n",
    "labels_valid_ohe = oh_enc.fit_transform(labels_valid_int)\n",
    "labels_train_ohe = oh_enc.fit_transform(labels_test_int)\n",
    "\n",
    "# uniques, labels_valid = np.unique(labels_valid, return_inverse=True)\n",
    "labels_train_cat = np_utils.to_categorical(labels_train_int)\n",
    "labels_valid_cat = np_utils.to_categorical(labels_valid_int)\n",
    "labels_test_cat  = np_utils.to_categorical(labels_test_int)\n",
    "\n",
    "#n_classes = labels_valid_ohe.shape[1]\n",
    "n_classes = 15\n",
    "# must hard-code this one\n",
    "\n",
    "print(\"Labels formats for convolutional layers:\")\n",
    "\n",
    "print(\"Train      int label format (?, ?, n_samples, n_channels)         | \", labels_train_int.shape)\n",
    "print(\"Validation int label format (?, ?, n_samples, n_channels)         | \", labels_valid_int.shape)\n",
    "print(\"Test       int label format (?, ?, n_samples, n_channels)         | \", labels_test_int.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pixels = data_train.shape[1] \n",
    "\n",
    "# Formatting data for convolutional layer:\n",
    "n_train_targets = data_train.shape[0]\n",
    "n_valid_targets = data_valid.shape[0]\n",
    "n_test_targets  = data_test.shape[0]\n",
    "\n",
    "#labels_train_int_4D = np.expand_dims(labels_train_int   , axis=0)\n",
    "#labels_train_int_4D = np.expand_dims(labels_train_int_4D, axis=0)\n",
    "#labels_valid_int_4D = np.expand_dims(labels_valid_int   , axis=0)\n",
    "#labels_valid_int_4D = np.expand_dims(labels_valid_int_4D, axis=0)\n",
    "#labels_test_int_4D  = np.expand_dims(labels_test_int    , axis=0)\n",
    "#labels_test_int_4D  = np.expand_dims(labels_test_int_4D , axis=0)\n",
    "\n",
    "\n",
    "print(\"Data formats for convolutional layers:\")\n",
    "\n",
    "print(\"Train      4D data format (n_samples,size_x, size_y, n_channels) | \", data_train.shape)\n",
    "print(\"Validation 4D data format (n_samples,size_x, size_y, n_channels) | \", data_valid.shape)\n",
    "print(\"Test       4D data format (n_samples,size_x, size_y, n_channels) | \", data_test.shape)\n",
    "\n",
    "#print(\"Train      4D label format (?, ?, n_samples, n_channels)         | \", labels_train_int_4D.shape)\n",
    "#print(\"Validation 4D label format (?, ?, n_samples, n_channels)         | \", labels_valid_int_4D.shape)\n",
    "#print(\"Test       4D label format (?, ?, n_samples, n_channels)         | \", labels_test_int_4D.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional 2D layers\n",
    "\n",
    "Not working with 'mean_squared_error' loss \n",
    "\n",
    "Loss function gets stuck when using 'softmax' activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best so far (97% accuracy on test):\n",
    "#\n",
    "#model_Conv = keras.Sequential([\n",
    "#                          keras.layers.Conv2D(8, kernel_size=(4,4), strides=(2,2), padding='same',\n",
    "#                                              activation=tf.nn.relu, input_shape=(n_pixels,n_pixels,1)),\n",
    "#                          keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n",
    "#                          keras.layers.Conv2D(64, kernel_size=(2,2), strides=(1,1), padding='same',\n",
    "#                                              activation=tf.nn.relu),\n",
    "#                          keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n",
    "#                          keras.layers.Conv2D(128, kernel_size=(2,2), strides=(1,1), padding='same',\n",
    "#                                              activation=tf.nn.relu),\n",
    "#                          keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n",
    "#                          #keras.layers.Dropout(0.3),\n",
    "#                          keras.layers.Flatten(),\n",
    "#                          keras.layers.Dense(10, activation=tf.nn.sigmoid),\n",
    "#                          keras.layers.Dropout(0.3),\n",
    "#                          keras.layers.Dense(n_classes, activation=tf.nn.sigmoid)\n",
    "#                          ])\n",
    "\n",
    "model_Conv = keras.Sequential([\n",
    "                          keras.layers.Conv2D(8, kernel_size=(4,4), strides=(1,1), padding='same',\n",
    "                                              activation=tf.nn.relu, input_shape=(n_pixels,n_pixels,n_bands),\n",
    "                                              data_format=\"channels_last\"),\n",
    "                          keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n",
    "                          keras.layers.Conv2D(64, kernel_size=(2,2), strides=(1,1), padding='same',\n",
    "                                              activation=tf.nn.relu),\n",
    "                          keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n",
    "                          #keras.layers.Conv2D(128, kernel_size=(2,2), strides=(1,1), padding='same',\n",
    "                          #                    activation=tf.nn.relu),\n",
    "                          #keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n",
    "                          ##keras.layers.Dropout(0.3),\n",
    "                          keras.layers.Flatten(),\n",
    "                          keras.layers.Dense(20, activation=tf.nn.sigmoid, input_shape=(784,)),\n",
    "                          keras.layers.Dropout(0.3),\n",
    "                          keras.layers.Dense(n_classes, activation=tf.nn.sigmoid)\n",
    "                          ])\n",
    "\n",
    "model_Conv.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.001), \n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model_Conv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_Conv = model_Conv.fit(data_train, labels_train_int, validation_data=(data_valid, labels_valid_int),\n",
    "                    epochs=10, batch_size=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# > Model evolution:\n",
    "plt.plot(history_Conv.history['loss'])\n",
    "plt.plot(history_Conv.history['val_loss'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# > Comparison with predictions:\n",
    "labels_pred_float_Conv = model_Conv.predict(data_test_4D)\n",
    "\n",
    "labels_pred_Conv = int_enc.inverse_transform(labels_pred_float_Conv.argmax(1))\n",
    "# reversing one hot encoding\n",
    "\n",
    "\n",
    "print(\"Classification report for %s:\\n%s\\n\"\n",
    "      % (model_Conv, metrics.classification_report(labels_test, labels_pred_Conv)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(labels_test, labels_pred_Conv))\n",
    "\n",
    "\n",
    "# > Plotting a few misclassified images (wrong labels):\n",
    "\n",
    "print('Examples of misclassified images:')\n",
    "\n",
    "indexes_wrong = np.nonzero(labels_pred_Conv != labels_test)\n",
    "# equivalent: indexes_wrong = [labels_pred_Conv != labels_test]\n",
    "# indexes of misclassified objects <indexed within range(data_test) or range(labels_pred_*)>\n",
    "\n",
    "labels_pred_int_Conv = labels_pred_float_Conv.argmax(1)\n",
    "\n",
    "data_test_wrong            = data_test[indexes_wrong]\n",
    "labels_test_int_wrong      = labels_test_int[indexes_wrong]\n",
    "labels_pred_int_Conv_wrong = labels_pred_int_Conv[indexes_wrong]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.subplots_adjust(hspace=0.25,wspace=0.05)\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.axis('off')\n",
    "    image_wrong = (data_test_wrong[i].reshape(n_pixels,n_pixels))\n",
    "    plt.imshow(image_wrong, cmap=plt.cm.gray_r, vmin=np.min(image_wrong), vmax=0.25*np.max(image_wrong), interpolation='nearest')\n",
    "    plt.title('label = %s | pred = %s' % (labels_test_int_wrong[i][0] , labels_pred_int_Conv_wrong[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/\n",
    "\n",
    "## 5) Using Tensorflow\n",
    "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/neural_network_raw.ipynb\n",
    "\n",
    "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/neural_network.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final check \n",
    "Using the test sample for a final check of the various algorithms used above. \n",
    "\n",
    "Checking if their performance is as good as it is reported in the validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"====================================================================================\")\n",
    "# Predictions for SVM:\n",
    "predicted = model_svc.predict(data_test)\n",
    "print(\"Classification report for %s:\\n%s\\n\"\n",
    "      % (model_svc, metrics.classification_report(labels_test, predicted)))\n",
    "print(\"Confusion matrix:\\n%s \\n\" % metrics.confusion_matrix(labels_test, predicted))\n",
    "print(\"====================================================================================\")\n",
    "\n",
    "# Predictions for LogisticRegression:\n",
    "predicted = model_svc.predict(data_test)\n",
    "print(\"Classification report for %s:\\n%s\\n\"\n",
    "      % (model_lrc, metrics.classification_report(labels_test, predicted)))\n",
    "print(\"Confusion matrix:\\n%s \\n\" % metrics.confusion_matrix(labels_test, predicted))\n",
    "print(\"====================================================================================\")\n",
    "\n",
    "# Predictions for MLP:\n",
    "predicted = model_MLP.predict(data_test)\n",
    "print(\"Classification report for %s:\\n%s\\n\"\n",
    "      % (model_MLP, metrics.classification_report(labels_test, predicted)))\n",
    "print(\"Confusion matrix:\\n%s \\n\" % metrics.confusion_matrix(labels_test, predicted))\n",
    "print(\"====================================================================================\")\n",
    "\n",
    "# Predictions for RandomForests:\n",
    "predicted = model_RF.predict(data_test)\n",
    "print(\"Classification report for %s:\\n%s\\n\"\n",
    "      % (model_RF, metrics.classification_report(labels_test, predicted)))\n",
    "print(\"Confusion matrix:\\n%s \\n\" % metrics.confusion_matrix(labels_test, predicted))\n",
    "print(\"====================================================================================\")\n",
    "\n",
    "# Predictions for Keras 1D Neural Network:\n",
    "labels_pred_float_1D = model_1D.predict(data_test)\n",
    "labels_pred_1D = int_enc.inverse_transform(labels_pred_float_1D.argmax(1))\n",
    "print(\"Classification report for %s:\\n%s\\n\"\n",
    "      % (model_1D, metrics.classification_report(labels_test, labels_pred_1D)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(labels_test, labels_pred_1D))\n",
    "print(\"====================================================================================\")\n",
    "\n",
    "# Predictions for Keras Convolutional Neural Network:\n",
    "labels_pred_float_Conv = model_Conv.predict(data_test_4D)\n",
    "labels_pred_Conv = int_enc.inverse_transform(labels_pred_float_Conv.argmax(1))\n",
    "print(\"Classification report for %s:\\n%s\\n\"\n",
    "      % (model_Conv, metrics.classification_report(labels_test, labels_pred_Conv)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(labels_test, labels_pred_Conv))\n",
    "print(\"====================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the trained models\n",
    "Using pickle we can save the trained models to use them in a later time w/o the need to re-train them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# saving the models to disk\n",
    "folder_saved = \"saved_models/v0.1\"\n",
    "\n",
    "model_labels = ['SVC','LogReg','MLP','RandFor','Ker_1D','Ker_Conv']\n",
    "models = ['model_SVC','model_LR','model_MLP','model_RF','model_1D','model_Conv']\n",
    "\n",
    "if not os.path.exists(folder_saved):\n",
    "    os.makedirs(folder_saved)\n",
    "    \n",
    "for modstr,model in zip(model_labels,models):\n",
    "    pickle.dump(model, open(folder_saved+'/galaxy_classification_SDSS_v1_'+modstr+'.sav', 'wb'))\n",
    "\n",
    "\n",
    "# To load the models at a later time:\n",
    "# loaded_model = pickle.load(open(filename, 'rb'))\n",
    "# result = loaded_model.score(X_test, Y_test)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
